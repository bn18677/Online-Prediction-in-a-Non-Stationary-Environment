---
title: "all in one plus intercept plus dependence"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library("MASS",character.only=TRUE)
library("Rfast")
library("stats")
set.seed(10)
```

First we build all the needed algorithms.

```{r}
alphast<-function(u,alpha,lamda,Rb){
  n<-length(alpha)
  store<-numeric(n)
  for (i in 1:n){
    store[i]<-sum((u^2)/((1+alpha[i]*lamda)^2))
  }
  return(store)
}

proj<-function(wt,covv,Rb,dim){
  decomp<-eigen(covv)
  V<-decomp$vectors
  Lamd<-decomp$values
  u<-t(V)%*%wt
  if (norm(wt,type="2")<=Rb){
    return(wt)
  }
  else{
    lamdmin<-min(Lamd)
    a<-((norm(u,type="2"))/(Rb-1))/lamdmin
    alpha<-seq(0,a,a/300)
    alpha<-alpha[binary_search(alphast(u,alpha,Lamd,Rb)[500:2000],0,TRUE)]
    return(solve((diag(dim))+alpha*covv)%*%wt)
  }
  return(w)
}
```


```{r}
ARCOR<-function(x,y,r,Rb,ll){
  dim<-length(x[1,])
  num<-length(x[,1])
  I<-diag(dim)
  tcov<-ll*I
  w<-t(matrix(0,1,dim))
  covv<-diag(dim)
  ey<-c()
  for (i in 1:num){
    denom<-r+t(x[i,])%*%covv%*%x[i,]
    numer<-t(x[i,])%*%w
    ey[i]<-numer
    wnum<-(y[i]-numer)
    wtilde<-w+(wnum/denom)[1,1]*covv%*%x[i,]
    covvtilde<-solve(solve(covv)+(x[i,]%*%t(x[i,])))
    if (min((as.vector(covv)-as.vector(tcov)))>=0){
      covv<-covvtilde
    }
    else {
      covv<-I
      tcov<-(ll*(4/5))*I
    }
    w<-proj(wtilde,covv,Rb,dim)
  }
  return(ey)
}

AROWR<-function(x,y,r){
  dim<-length(x[1,])
  num<-length(x[,1])
  w<-t(matrix(0,1,dim))
  covv<-diag(dim)
  ey<-c()
  for (i in 1:num){
    ey[i]<-t(x[i,])%*%w
    wnum<-(y[i]-ey[i])
    wden<-r+t(x[i,])%*%covv%*%x[i,]
    w<-w+(wnum/wden)[1,1]*covv%*%x[i,]
    covv<-solve(solve(covv)+(1/r)*(x[i,]%*%t(x[i,])))
  }
  return(ey)
}
```

````{r}
LASER<-function(x,y,b,c){
  dim<-length(x[1,])
  num<-length(x[,1])
  w<-t(matrix(0,1,dim))
  I<-diag(dim)
  covv<-I*((c-b)/(b*c))
  ey<-c()
  for (i in 1:num){
    denom<-1+t(x[i,])%*%(covv+((1/c)*I))%*%x[i,]
    numer<-t(x[i,])%*%w
    ey[i]<-numer/denom
    wnum<-(y[i]-numer)
    w<-w+(wnum/denom)[1,1]*(covv+((1/c)*I))%*%x[i,]
    covv<-solve(solve((covv+((1/c)*I)))+(x[i,]%*%t(x[i,])))
  }
  return(ey)
}

AAR<-function(x,y,b){
  dim<-length(x[1,])
  num<-length(x[,1])
  w<-t(matrix(0,1,dim))
  covv<-diag(dim)*(1/b)
  ey<-c()
  for (i in 1:num){
    denom<-1+t(x[i,])%*%covv%*%x[i,]
    numer<-t(x[i,])%*%w
    ey[i]<-numer/denom
    wnum<-(y[i]-numer)
    w<-w+(wnum/denom)[1,1]*covv%*%x[i,]
    covv<-solve(solve(covv)+(x[i,]%*%t(x[i,])))
  }
  return(ey)
}
```

```{r}
CRRLS<-function(x,y,r,t0){
  dim<-length(x[1,])
  num<-length(x[,1])
  w<-t(matrix(0,1,dim))
  covv<-diag(dim)
  ey<-c()
  for (i in 1:num){
    ey[i]<-t(x[i,])%*%w
    wnum<-(y[i]-ey[i])
    wden<-r+t(x[i,])%*%covv%*%x[i,]
    w<-w+(wnum/wden)[1,1]*covv%*%x[i,]
    covv<-solve(r*solve(covv)+(x[i,]%*%t(x[i,])))
    if (i%%t0==0){
      covv<-diag(dim)
    }
  }
  return(ey)
}

RLS<-function(x,y,r){
  dim<-length(x[1,])
  num<-length(x[,1])
  w<-t(matrix(0,1,dim))
  covv<-diag(dim)
  ey<-c()
  for (i in 1:num){
    ey[i]<-t(x[i,])%*%w
    wnum<-(y[i]-ey[i])
    wden<-r+t(x[i,])%*%covv%*%x[i,]
    w<-w+(wnum/wden)[1,1]*covv%*%x[i,]
    covv<-solve(r*solve(covv)+(x[i,]%*%t(x[i,])))
  }
  return(ey)
}
```

```{r}
cumsqloss<-function(y,yh,burn){
  Tt<-length(y)
  return(sum((y[(burn+1):Tt]-yh[(burn+1):Tt])^2))
}

cumsqloss2<-function(y,yh){
  Tt<-length(y)
  tot<-0
  store<-numeric(Tt)
  for (i in (1):Tt){
    tot<-tot+(yh[i]-y[i])^2
    store[i]<-tot
  }
  return(store)
}
```

Now we load the real-world data.

```{r}
temps<-read.csv("temperature_history.csv")
loads<-read.csv("Load_history.csv")
```

We now turn the real-world data into a useable matrix and add an intercept to the xmat which corresponds to the weights $\mathbf{x}_{t}$ that we will use to make our predictions. We also scale the matrices. Further we add another column to xmat. This column includes the past observation data to ensure that is included when we make the predictions.

```{r}
ymat<-cbind()
for (i in 1:20){
  cursuby<-t(data.matrix(subset(loads[loads$zone_id==i,],select=-c(zone_id,year,month,day))))
  cursuby<-cursuby[1:(length(cursuby)-(8*24))]
  ymat<-cbind(ymat,c(cursuby))
}
ymat<-ymat/10000

```

```{r}
xmat<-cbind()
for (i in 1:11){
  cursubx<-t(data.matrix(subset(temps[temps$station_id==i,],select=-c(station_id,year,month,day))))
  cursubx<-cursubx[1:(length(cursubx)-24)]
  xmat<-cbind(xmat,c(cursubx))
}
numrow<-length(xmat[,1])
xmat<-cbind(xmat/10,matrix(rep(1,numrow),ncol=1))
```

```{r}
entnum<-length(ymat[,1])
```

We now ensure make the zone we are estimating the energy load for change every half a year.

```{r}
i<-1
hy<-ceiling((365/2)*24)
y<-numeric(length(ymat[,1]))
zone<-100
zone1<-zone
while (zone==zone1){
  zone<-sample(1:20,1)
}
zone1<-zone
while (i<entnum) {
  if ((i+hy-1)>entnum){
    hy<-entnum-i+1
  }
  y[i:(i+hy-1)]<-(ymat[,zone])[i:(i+hy-1)]
  i<-i+hy
}
length(y)
length(y)-length(y[!is.na(y)])
```

We now ensure any empty elements of the matrix are removed 

```{r}
naelsy<-c()
for (i in 1:length(y)){
  if (is.na(y[i])==TRUE){
    naelsy<-append(naelsy,i)
  }
}
length(naelsy)
```

```{r}
y<-y[!is.na(y)]
x<-xmat[-c(naelsy),]
x<-cbind(x,matrix(c(0,head(y,-1)),ncol=1))
num<-length(y)
num
head(x)
tail(x)
```

We now use the first 5000 points in the training data and test different parameters for LASER on this training data to help us guess what the optimum parameters will be by seeing which has the smallest cumulative error.

```{r}
cumsqloss(y[500:2000],LASER(x,y,1,5000)[500:2000],0)
cumsqloss(y[500:2000],LASER(x,y,1,50000)[500:2000],0)
cumsqloss(y[500:2000],LASER(x,y,1,500000)[500:2000],0)
cumsqloss(y[500:2000],LASER(x,y,1,5000000)[500:2000],0)
cumsqloss(y[500:2000],LASER(x,y,1,50000000)[500:2000],0)
cumsqloss(y[500:2000],LASER(x,y,1,500000000)[500:2000],0)
cumsqloss(y[500:2000],LASER(x,y,1,50000000000)[500:2000],0)
cumsqloss(y[500:2000],LASER(x,y,1,1000000000000)[500:2000],0)
cumsqloss(y[500:2000],LASER(x,y,1,3000000000000)[500:2000],0)
cumsqloss(y[500:2000],LASER(x,y,1,4000000000000)[500:2000],0)
```

We now run LASER and AAR on the full data set and analyse them accordingly.

```{r}
b<-1
c<-100000000
b2<-(b*c)/(c-b)

ytest<-y[2001:num]

yhatLASER<-LASER(x,y,b,c)[2001:num]
yhatAAR<-AAR(x,y,b2)[2001:num]

cumsqloss(ytest,yhatLASER,0)/(num-2000)
cumsqloss(ytest,yhatAAR,0)/(num-2000)

plot((1:(num-2000)),(ytest-yhatAAR))
plot((1:(num-2000)),(ytest-yhatLASER))

cumAAR<-cumsqloss2(ytest,yhatAAR)
cumLASER<-cumsqloss2(ytest,yhatLASER)
plot((1:(num-2000)),cumAAR,type="l",col="red")
lines((1:(num-2000)),cumLASER,col="green")
```

We now use the first 5000 points in the training data and test different parameters for ARCOR and AROWR on this training data to help us guess what the optimum parameters will be by seeing which has the smallest cumulative error.

```{r}
cumsqloss(y[500:2000],ARCOR(x,y,0.000000001,10000,0.95)[500:2000],0)
cumsqloss(y[500:2000],ARCOR(x,y,0.000001,10000,0.95)[500:2000],0)
cumsqloss(y[500:2000],ARCOR(x,y,0.001,10000,0.95)[500:2000],0)
cumsqloss(y[500:2000],ARCOR(x,y,0.003,10000,0.95)[500:2000],0)
cumsqloss(y[500:2000],ARCOR(x,y,0.004,10000,0.95)[500:2000],0)
cumsqloss(y[500:2000],ARCOR(x,y,0.005,10000,0.95)[500:2000],0)
cumsqloss(y[500:2000],ARCOR(x,y,100,10000,0.95)[500:2000],0)
```

```{r}
cumsqloss(y[500:2000],AROWR(x,y,0.5)[500:2000],0)
cumsqloss(y[500:2000],AROWR(x,y,2.4)[500:2000],0)
cumsqloss(y[500:2000],AROWR(x,y,2.5)[500:2000],0)
cumsqloss(y[500:2000],AROWR(x,y,2.6)[500:2000],0)
cumsqloss(y[500:2000],AROWR(x,y,2.7)[500:2000],0)
cumsqloss(y[500:2000],AROWR(x,y,2.8)[500:2000],0)
cumsqloss(y[500:2000],AROWR(x,y,2.9)[500:2000],0)
cumsqloss(y[500:2000],AROWR(x,y,100)[500:2000],0)
```

We now run ARCOR and AROWR on the full data set and analyse them accordingly.

```{r}
yhatARCOR<-ARCOR(x,y,0.000001,10000,0.95)[2001:num]
yhatAROWR<-AROWR(x,y,2.6)[2001:num]

ytest<-y[2001:num]

cumsqloss(ytest,yhatARCOR,0)/(num-2000)
cumsqloss(ytest,yhatAROWR,0)/(num-2000)

plot((1:(num-2000)),(ytest-yhatAROWR))
plot((1:(num-2000)),(ytest-yhatARCOR))

cumAROWR<-cumsqloss2(ytest,yhatAROWR)
cumARCOR<-cumsqloss2(ytest,yhatARCOR)
plot((1:(num-2000)),cumAROWR,type="l",col="red")
lines((1:(num-2000)),cumARCOR,col="green")
```

We now use the first 5000 points in the training data and test different parameters for CR-RLS on this training data to help us guess what the optimum parameters will be by seeing which has the smallest cumulative error.

```{r}
cumsqloss(y[500:2000],CRRLS(x,y,0.9,1)[500:2000],0)
cumsqloss(y[500:2000],CRRLS(x,y,0.9,2)[500:2000],0)
cumsqloss(y[500:2000],CRRLS(x,y,0.9,3)[500:2000],0)
cumsqloss(y[500:2000],CRRLS(x,y,0.9,4)[500:2000],0)
cumsqloss(y[500:2000],CRRLS(x,y,0.9,5)[500:2000],0)
cumsqloss(y[500:2000],CRRLS(x,y,0.9,10)[500:2000],0)
cumsqloss(y[500:2000],CRRLS(x,y,0.9,15)[500:2000],0)
cumsqloss(y[500:2000],CRRLS(x,y,0.9,20)[500:2000],0)
cumsqloss(y[500:2000],CRRLS(x,y,0.9,25)[500:2000],0)
cumsqloss(y[500:2000],CRRLS(x,y,0.9,30)[500:2000],0)
cumsqloss(y[500:2000],CRRLS(x,y,0.9,35)[500:2000],0)
cumsqloss(y[500:2000],CRRLS(x,y,0.9,40)[500:2000],0)
cumsqloss(y[500:2000],CRRLS(x,y,0.9,45)[500:2000],0)
cumsqloss(y[500:2000],CRRLS(x,y,0.9,50)[500:2000],0)
cumsqloss(y[500:2000],CRRLS(x,y,0.9,2000)[500:2000],0)

```

We now run CR-RLS and RLS on the full data set and analyse them accordingly.

```{r}
yhatCRRLS<-CRRLS(x,y,0.9,2)[2001:num]
yhatRLS<-RLS(x,y,0.9)[2001:num]

ytest<-y[2001:num]

cumsqloss(ytest,yhatCRRLS,0)/(num-2000)
cumsqloss(ytest,yhatRLS,0)/(num-2000)

plot((1:(num-2000)),(ytest-yhatRLS))
plot((1:(num-2000)),(ytest-yhatCRRLS))

cumRLS<-cumsqloss2(ytest,yhatRLS)
cumCRRLS<-cumsqloss2(ytest,yhatCRRLS)

plot((1:(num-2000)),cumRLS,type="l",col="red")
lines((1:(num-2000)),cumCRRLS,col="green")
```

We now plot all cumulative squared errors on one glass for easy comparison.

```{r}
plot((1:(num-2000)),cumRLS,type="l",col="red",ylab="Cumulative Squared Error",xlab="Number of iterations")
lines((1:(num-2000)),cumCRRLS,col="orange")
lines((1:(num-2000)),cumARCOR,col="purple")
lines((1:(num-2000)),cumAROWR,col="blue")
lines((1:(num-2000)),cumAAR,col="darkgreen")
lines((1:(num-2000)),cumLASER,col="green")
legend(0, 18500, legend=c("RLS", "CR-RLS","ARCOR", "AROWR","AAR", "LASER"),
       col=c("red", "orange","purple", "blue","darkgreen", "green"), lty=1:2, cex=0.8)
```